# ğŸš€ AI Studio - Production-Ready Multimodal AI Platform

A comprehensive, production-ready AI studio built with FastAPI, Vue.js, and state-of-the-art small language models. Features multimodal capabilities, RAG (Retrieval-Augmented Generation), fine-tuning, agent orchestration, and real-time processing.

## âœ¨ Features

### ğŸ¤– **Multi-Agent Architecture**
- **Code Agent**: Specialized code generation, review, and explanation
- **Text Agent**: Creative writing, summarization, and analysis  
- **RAG Agent**: Document-based question answering with context retrieval
- **Agent Orchestrator**: Intelligent task distribution and load balancing

### ğŸ§  **Advanced AI Capabilities**
- **Small, Efficient Models**: Phi-2, T5-Small, MiniLM for optimal performance
- **Fine-tuning Support**: LoRA-based fine-tuning for domain specialization
- **Quantization**: 4-bit quantization for memory efficiency
- **Domain Specialization**: Code, creative, analysis, summarization domains

### ğŸ” **RAG (Retrieval-Augmented Generation)**
- **Document Ingestion**: PDF, DOCX, TXT, JSON support
- **Vector Search**: FAISS-based semantic search
- **Context Enhancement**: Automatic context retrieval for queries
- **Citation Tracking**: Source attribution for generated responses

### ğŸ™ï¸ **Multimodal Processing**
- **Voice-to-Text**: Whisper-based speech recognition
- **Image Analysis**: CLIP-based image understanding
- **Audio Processing**: Real-time transcription
- **File Upload**: Multi-format document processing

### ğŸ› ï¸ **Development Tools**
- **Code Canvas**: Interactive code editor with live preview
- **Template System**: Pre-built prompts for common tasks
- **Streaming Responses**: Real-time response generation
- **Model Switching**: Dynamic model selection

### ğŸ“Š **Production Features**
- **Monitoring**: Prometheus metrics and Grafana dashboards
- **Logging**: Structured logging with correlation IDs
- **Health Checks**: Comprehensive service health monitoring
- **Auto-scaling**: Container orchestration ready
- **Backup/Restore**: Automated data backup systems

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚   AI Services   â”‚
â”‚   (Vue.js)      â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (PyTorch)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx         â”‚    â”‚   PostgreSQL    â”‚    â”‚   Redis Cache   â”‚
â”‚   (Proxy)       â”‚    â”‚   (Metadata)    â”‚    â”‚   (Sessions)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monitoring    â”‚    â”‚   Task Queue    â”‚    â”‚   File Storage  â”‚
â”‚   (Grafana)     â”‚    â”‚   (Celery)      â”‚    â”‚   (Volumes)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Git
- 4GB+ RAM recommended
- NVIDIA GPU (optional, for acceleration)

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/ai-studio.git
cd ai-studio
```

### 2. Development Setup
```bash
# Make scripts executable
chmod +x scripts/*.sh

# Start development environment
./scripts/start_dev.sh
```

### 3. Production Deployment
```bash
# Deploy with Docker Compose
./scripts/deploy_production.sh
```

### 4. Access the Application
- **Frontend**: http://localhost:3000
- **API Docs**: http://localhost:8000/docs
- **Monitoring**: http://localhost:3001 (Grafana)
- **Task Monitor**: http://localhost:5555 (Flower)

## ğŸ“– Usage Guide

### Basic Chat
```python
# Send a message
POST /api/chat-text/
{
    "messages": [{"role": "user", "content": "Write a Python function"}],
    "domain": "code",
    "temperature": 0.7
}
```

### RAG-Enhanced Chat
```python
# Upload documents first
POST /api/chat-rag/upload-documents
# Form data with files

# Query with context
POST /api/chat-rag/
{
    "messages": [{"role": "user", "content": "What does the document say about AI?"}],
    "use_rag": true
}
```

### Voice Processing
```python
# Transcribe audio
POST /api/voice/transcribe
# Form data with audio file
```

### Fine-tuning
```python
# Start fine-tuning job
POST /api/fine-tuning/
{
    "adapter_name": "my_specialist",
    "training_data": [...],
    "lora_config": {"r": 16, "lora_alpha": 32}
}
```

## ğŸ”§ Configuration

### Environment Variables

#### Backend (.env)
```bash
# Server
HOST=0.0.0.0
PORT=8000
WORKERS=1
ENVIRONMENT=production

# Database
DATABASE_URL=postgresql://user:pass@localhost/ai_studio

# Redis
REDIS_URL=redis://localhost:6379/0

# AI Models
MODEL_CACHE_DIR=./models
RAG_STORAGE_DIR=./rag_storage

# Security
SECRET_KEY=your-secret-key
ALLOWED_ORIGINS=http://localhost:3000

# Monitoring
ENABLE_METRICS=true
LOG_LEVEL=INFO
```

#### Frontend (.env)
```bash
VITE_API_BASE_URL=http://localhost:8000
VITE_APP_NAME=AI Studio
VITE_APP_VERSION=1.0.0
```

### Model Configuration

The system uses optimized small models:

- **Text Generation**: Microsoft Phi-2 (2.7B parameters)
- **Code Generation**: CodeT5p-220M
- **Embeddings**: all-MiniLM-L6-v2
- **Speech**: Whisper-tiny
- **Vision**: CLIP-ViT-B-16

## ğŸ“Š Monitoring

### Health Checks
```bash
# Check system health
curl http://localhost:8000/health

# Check model status
curl http://localhost:8000/models/status

# Check metrics
curl http://localhost:8000/metrics
```

### Grafana Dashboards

Pre-configured dashboards for:
- API Response Times
- Model Inference Metrics
- System Resource Usage
- Error Rates and Success Rates
- Agent Performance

## ğŸ› ï¸ Development

### Project Structure
```
ai-studio/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic
â”‚   â”‚   â”œâ”€â”€ models/       # Data models
â”‚   â”‚   â””â”€â”€ core/         # Configuration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # Vue components
â”‚   â”‚   â”œâ”€â”€ views/        # Pages
â”‚   â”‚   â”œâ”€â”€ store/        # State management
â”‚   â”‚   â””â”€â”€ api/          # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ scripts/              # Deployment scripts
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

### Adding New Models
1. Update `ModelManager` in `services/model_manager.py`
2. Add model configuration
3. Implement loading and inference logic
4. Update API endpoints

### Adding New Agents
1. Extend `BaseAgent` class
2. Implement required methods
3. Register with `AgentOrchestrator`
4. Add API endpoints

### Custom Fine-tuning
```python
from app.services.llm import llm_service

# Prepare training data
training_data = [
    {
        "messages": [
            {"role": "user", "content": "input"},
            {"role": "assistant", "content": "output"}
        ],
        "domain": "custom"
    }
]

# Start fine-tuning
result = await llm_service.fine_tune_lora(
    training_data=training_data,
    adapter_name="custom_adapter",
    lora_config={
        "r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1
    }
)
```

## ğŸ” Security

### Authentication
- JWT-based authentication (optional)
- API key authentication
- Rate limiting per IP/user

### Data Protection
- Input sanitization
- SQL injection prevention
- XSS protection
- CORS configuration

### Model Security
- Model weight validation
- Inference timeout limits
- Memory usage monitoring
- Sandboxed execution

## ğŸ“ˆ Performance

### Optimization Features
- **Model Quantization**: 4-bit quantization reduces memory by 75%
- **Caching**: Redis-based response caching
- **Batching**: Request batching for efficiency
- **Load Balancing**: Multi-worker deployment
- **Memory Management**: Automatic cleanup and optimization

### Benchmarks
- **Response Time**: <2s for most queries
- **Memory Usage**: <4GB with quantization
- **Throughput**: 10+ concurrent requests
- **Model Loading**: <30s cold start

## ğŸ› Troubleshooting

### Common Issues

#### Backend not starting
```bash
# Check logs
./scripts/logs.sh api

# Verify models are downloading
docker-compose exec api ls -la /app/models
```

#### Out of memory
```bash
# Enable quantization
export ENABLE_QUANTIZATION=true

# Reduce batch size
export MAX_BATCH_SIZE=1
```

#### RAG not working
```bash
# Check document upload
curl -X POST -F "files=@document.pdf" http://localhost:8000/api/chat-rag/upload-documents

# Verify vector index
./scripts/shell.sh api
python -c "from app.services.rag_engine import rag_engine; print(rag_engine.get_stats())"
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

### Development Setup
```bash
# Backend
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Frontend
cd frontend
npm install
npm run dev
```

### Testing
```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm run test
```

## ğŸ“ License

This project is licensed under the MIT License 

## ğŸ™ Acknowledgments

- **Hugging Face** for transformer models and libraries
- **FastAPI** for the excellent web framework
- **Vue.js** for the reactive frontend framework
- **Microsoft** for the Phi-2 model
- **OpenAI** for Whisper and CLIP models


---

**Built with â¤ï¸ for the AI community**