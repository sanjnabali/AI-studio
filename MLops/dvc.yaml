# DVC Pipeline for AI Studio MLOps
# This file defines the machine learning pipeline stages for model training,
# evaluation, and deployment in the AI Studio platform.

stages:
  # Data preparation and preprocessing
  data_preparation:
    cmd: python scripts/prepare_data.py
    deps:
      - scripts/prepare_data.py
      - data/raw/
      - params.yaml
    outs:
      - data/processed/train_data.jsonl
      - data/processed/validation_data.jsonl
      - data/processed/test_data.jsonl
      - data/processed/dataset_stats.json
    params:
      - data_preparation
    metrics:
      - metrics/data_stats.json
    plots:
      - plots/data_distribution.json

  # Feature extraction and embedding generation
  feature_extraction:
    cmd: python scripts/extract_features.py
    deps:
      - scripts/extract_features.py
      - data/processed/train_data.jsonl
      - data/processed/validation_data.jsonl
      - params.yaml
    outs:
      - data/features/train_embeddings.npy
      - data/features/validation_embeddings.npy
      - data/features/feature_metadata.json
    params:
      - feature_extraction
    metrics:
      - metrics/feature_stats.json

  # Base model training (Foundation models)
  train_base_models:
    cmd: python scripts/train_base_models.py
    deps:
      - scripts/train_base_models.py
      - data/processed/train_data.jsonl
      - data/processed/validation_data.jsonl
      - params.yaml
    outs:
      - models/base/phi2_base/
      - models/base/codet5_base/
      - models/base/t5_base/
      - models/training_logs/base_training.log
    params:
      - base_model_training
    metrics:
      - metrics/base_training_metrics.json
    plots:
      - plots/base_training_curves.json

  # Domain-specific fine-tuning
  finetune_domain_models:
    cmd: python scripts/finetune_domain_models.py
    deps:
      - scripts/finetune_domain_models.py
      - models/base/phi2_base/
      - models/base/codet5_base/
      - models/base/t5_base/
      - data/processed/train_data.jsonl
      - data/processed/validation_data.jsonl
      - params.yaml
    outs:
      - models/domain/code_specialist/
      - models/domain/creative_specialist/
      - models/domain/analysis_specialist/
      - models/domain/summarization_specialist/
      - models/training_logs/domain_finetuning.log
    params:
      - domain_finetuning
    metrics:
      - metrics/domain_finetuning_metrics.json
    plots:
      - plots/domain_finetuning_curves.json

  # LoRA adapter training
  train_lora_adapters:
    cmd: python scripts/train_lora_adapters.py
    deps:
      - scripts/train_lora_adapters.py
      - models/base/phi2_base/
      - data/processed/train_data.jsonl
      - data/processed/validation_data.jsonl
      - params.yaml
    outs:
      - models/adapters/lora/code_lora/
      - models/adapters/lora/creative_lora/
      - models/adapters/lora/analysis_lora/
      - models/adapters/lora/summary_lora/
    params:
      - lora_training
    metrics:
      - metrics/lora_training_metrics.json

  # Model quantization and optimization
  quantize_models:
    cmd: python scripts/quantize_models.py
    deps:
      - scripts/quantize_models.py
      - models/domain/code_specialist/
      - models/domain/creative_specialist/
      - models/domain/analysis_specialist/
      - models/domain/summarization_specialist/
      - params.yaml
    outs:
      - models/quantized/code_specialist_4bit/
      - models/quantized/creative_specialist_4bit/
      - models/quantized/analysis_specialist_4bit/
      - models/quantized/summarization_specialist_4bit/
    params:
      - quantization
    metrics:
      - metrics/quantization_metrics.json

  # Model evaluation and benchmarking
  evaluate_models:
    cmd: python scripts/evaluate_models.py
    deps:
      - scripts/evaluate_models.py
      - models/domain/
      - models/quantized/
      - models/adapters/lora/
      - data/processed/test_data.jsonl
      - params.yaml
    outs:
      - evaluation/results/
      - evaluation/benchmarks/
      - evaluation/reports/
    params:
      - evaluation
    metrics:
      - metrics/evaluation_metrics.json
      - metrics/benchmark_results.json
    plots:
      - plots/model_comparison.json
      - plots/performance_metrics.json

  # RAG system training and optimization
  train_rag_system:
    cmd: python scripts/train_rag_system.py
    deps:
      - scripts/train_rag_system.py
      - data/rag/documents/
      - models/base/
      - params.yaml
    outs:
      - models/rag/embeddings/
      - models/rag/vector_store/
      - models/rag/retrieval_config.json
    params:
      - rag_training
    metrics:
      - metrics/rag_metrics.json

  # Agent training and optimization
  train_agents:
    cmd: python scripts/train_agents.py
    deps:
      - scripts/train_agents.py
      - models/domain/
      - models/quantized/
      - data/agent_training/
      - params.yaml
    outs:
      - models/agents/code_agent/
      - models/agents/text_agent/
      - models/agents/rag_agent/
      - models/agents/summarizer_agent/
      - models/agents/voice_agent/
    params:
      - agent_training
    metrics:
      - metrics/agent_performance.json

  # Model validation and testing
  validate_models:
    cmd: python scripts/validate_models.py
    deps:
      - scripts/validate_models.py
      - models/domain/
      - models/quantized/
      - models/adapters/lora/
      - data/validation/
      - params.yaml
    outs:
      - validation/reports/
      - validation/test_results/
    params:
      - validation
    metrics:
      - metrics/validation_metrics.json
    plots:
      - plots/validation_results.json

  # Performance benchmarking
  benchmark_performance:
    cmd: python scripts/benchmark_performance.py
    deps:
      - scripts/benchmark_performance.py
      - models/domain/
      - models/quantized/
      - params.yaml
    outs:
      - benchmarks/performance_results.json
      - benchmarks/latency_analysis.json
      - benchmarks/memory_analysis.json
    params:
      - benchmarking
    metrics:
      - metrics/performance_benchmarks.json
    plots:
      - plots/performance_comparison.json

  # Model packaging and containerization
  package_models:
    cmd: python scripts/package_models.py
    deps:
      - scripts/package_models.py
      - models/quantized/
      - models/adapters/lora/
      - models/rag/
      - models/agents/
      - params.yaml
    outs:
      - deployment/model_packages/
      - deployment/docker_images/
      - deployment/deployment_configs/
    params:
      - packaging

  # Deployment preparation
  prepare_deployment:
    cmd: python scripts/prepare_deployment.py
    deps:
      - scripts/prepare_deployment.py
      - deployment/model_packages/
      - deployment/deployment_configs/
      - params.yaml
    outs:
      - deployment/staging/
      - deployment/production_ready/
      - deployment/health_checks/
    params:
      - deployment
    metrics:
      - metrics/deployment_readiness.json

# Experiment tracking and metrics collection
vars:
  - experiment_name: ${EXPERIMENT_NAME}
  - model_version: ${MODEL_VERSION}
  - timestamp: ${TIMESTAMP}

# Data version control
artifacts:
  models:
    path: models/
    type: model
    desc: Trained AI models and adapters
  
  datasets:
    path: data/
    type: dataset  
    desc: Training and evaluation datasets
  
  evaluations:
    path: evaluation/
    type: evaluation
    desc: Model evaluation results and benchmarks

# Remote storage configuration
remote:
  - name: s3_models
    url: s3://ai-studio-models/${experiment_name}/
    config:
      credentialpath: .aws/credentials
      region: us-west-2
  
  - name: azure_backup
    url: azure://ai-studio-backup/models/
    config:
      connection_string: ${AZURE_CONNECTION_STRING}

# Pipeline validation rules
validation:
  accuracy_threshold: 0.85
  latency_threshold_ms: 2000
  memory_limit_gb: 4
  min_test_coverage: 0.9

# Monitoring and alerting
monitoring:
  metrics:
    - accuracy
    - latency
    - memory_usage
    - throughput
  
  alerts:
    - condition: accuracy < 0.8
      action: notify_team
    - condition: latency > 3000
      action: auto_rollback
    - condition: memory_usage > 0.9
      action: scale_down

# CI/CD integration
ci_cd:
  triggers:
    - data_update
    - model_update
    - config_change
  
  tests:
    - unit_tests
    - integration_tests
    - performance_tests
    - security_tests
  
  deployment_stages:
    - development
    - staging  
    - production

# Reproducibility settings
reproducibility:
  seed: 42
  python_version: "3.9"
  cuda_version: "11.8"
  framework_versions:
    torch: "2.1.0"
    transformers: "4.35.0"
    datasets: "2.14.0"